---
title: "Analysing Covid-19 and its impact over brazilian municipalities in terms of socio-economic ambiance indicators"
author: "Rodrigo Silva (@lais.huol.ufrn.br)"
date: "November 20th, 2020"
output:
  html_notebook:
    highlight: tango
    mathjax: null
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

# Briefing

This analysis is part of a challenge from a Machine Learning Class where the aim is to apply the concepts of Data Analysis and Machine Learning. This project has been took by 3 PhD candidates (Vera, Júlio and Rodrigo).

In this notebook we intend to analyse socio-economic indicators from all brazilian cities and the number of cases  and deaths of Covid19 and find which variable could help us to understand the dynamics behind this disease. We collected almost 60 indicators corresponding to education, GDP per capita, basic sanitation, HDI, Gini index, life expectancy, mortality and others related.

For the first step of this project, we'll look for the relation among all those indicators, how they are related between them and so select the most qualified to use some machine learning approaches to classify the data.  

# Setup

The R programming machine learning **caret** package (Classification And REgression Training) holds tons of functions that helps to build predictive models. It holds tools for data splitting, pre-processing, feature selection, tuning and supervised – unsupervised learning algorithms, etc. It is similar to **scikit-learn** library in python.

## Libraries

```{r libraries, message=FALSE, warning=FALSE}
library(dplyr)
library(plotly)
library(tidyr)
library(Hmisc)
library(taRifx)
library(modeest) 
library(caret)
library(e1071)
library(DMwR)
library(rpart.plot)
```

## Data

```{r dataset}
dataset <- read.csv2('dataset_02.csv', header = T)
dataset_dict <- read.csv2('dicionário de dados.csv', header = T)
cobertura_ab <- read.csv2('cobertura_ab.csv', header = T)
```

# Exploratory Analysis

## Data Description

The head of dataset.

```{r}
head(dataset)
```

The head of dataset dictionary.

```{r}
head(dataset_dict)
```
Renaming the variables with names more descriptive.

```{r}
names(dataset) <- dataset_dict$nome
```

Checking the type of each variable.

```{r}
str(dataset)
```

As we can see a big part of the data is missing the real type. Where it's supposed to be `numeric` we have `character` so we need to fix this.

To fix all misreading of types we gonna cast the variable `data` to date type and what actually is `character` to `factor`. Therefore, we can run an automatic cast to all `numeric` variables that are marked as `charcater`.

First step: converting what is not numeric.

```{r}
dataset$data <- as.Date(dataset$data, "%Y-%m-%d")

dataset$nome_municipio <- as.factor(dataset$nome_municipio)
dataset$nome_estado <- as.factor(dataset$nome_estado)
dataset$nome_municipio_2 <- as.factor(dataset$nome_municipio_2)
dataset$nome_regiao <- as.factor(dataset$nome_regiao)
dataset$porte_municipio <-as.factor(dataset$porte_municipio)
```

Second step: converting what is supposed to be numeric.

```{r warning=FALSE}
dataset <- japply( dataset, which(sapply(dataset, class)=="character"), as.numeric )
```

Checking

```{r}
str(dataset)
```

Some variables is not important to our goal, duplicated or with aggregate values to the state, not to municipality, like `data`, `nome_municipio_2` and `populacao_total_pnud`, thus we need to remove them. 
Seizing the opportunity we gonna reorder the columns.

```{r}
dataset <- dataset %>%
  select(cod_ibge_municipio, longitude, latitude, nome_municipio, nome_estado, nome_regiao, porte_municipio, capital, regiao_metropolitana, populacao_total, everything(), 
         -data, -nome_municipio_2, -populacao_total_pnud)
```

If we check the dataset all the columns after the `total_obitos_confirmados_covid` and before `tx_incidencia_sol_media_anual` are values aggregated to the state so doesn't bring to much significance to the municipalities analysis.
On the other hand, the last 13 columns are data about light incidence which has no correlation with Covid19, so we gonna cut it off from dataset.

```{r}
dataset <- dataset[, 1:35]
```

Now I gonna combine the dataset with data about primary care coverage. I believe that could be a great indicator about how the public health care system was able to handle the pandemic situation.

First of all, we should rename the columns to attend the pattern adopted in this notebook and we gonna discard the variable `População` as we already have this one in dataset.

```{r}
cobertura_ab <- cobertura_ab %>%
  select(IBGE, Cobertura.AB) %>%
  rename(cod_ibge_municipio = IBGE,
         cobertura_ab = Cobertura.AB)
```

Now, merging process:

```{r}
dataset <- merge(dataset,
                 cobertura_ab,
                 by = 'cod_ibge_municipio',
                 all.x = T) # this means we should keep mostly all data from dataset 
```

Now, We will create two new variables about Covid19 in the municipalities. The first one will be `incidence` which able us to compare the situation between cities. The second one will be `lethality` which tell us how hard the pandemic is being in that location and indirectly how the public health system is being responsive.

```{r}
dataset$incidencia <- (dataset$total_casos_confirmados_covid/dataset$populacao_total)*100000
dataset$letalidade <- (dataset$total_obitos_confirmados_covid/dataset$total_casos_confirmados_covid)*100
```


Check the last version of the dataset we gonna use:

```{r}
str(dataset)
```

Now we have our dataset all organized, standardized and cleansed. Let's move on!

## Data Frequency Distribution

Here we gonna analyze the distribution of the values. This is a important step as this is where we can understand how the data is distributed by itself.
So we gonna prepare some histograms to understand a little about the data. The seven first variables doesn't need a histogram since they actually are qualitative features.

```{r}
listOfPlots <- list()

listOfPlots[["capital"]] <- plotly_build(plot_ly(x = dataset$capital, type = "histogram", name = "capital"))
listOfPlots[["regiao_metropolitana"]] <- plotly_build(plot_ly(x = dataset$regiao_metropolitana, type = "histogram", name = "regiao_metropolitana"))
listOfPlots[["populacao_total"]] <- plotly_build(plot_ly(x = dataset$populacao_total, type = "histogram", name = "populacao_total"))
listOfPlots[["perc_matricula_epf"]] <- plotly_build(plot_ly(x = dataset$perc_matricula_epf, type = "histogram", name = "perc_matricula_epf"))
listOfPlots[["perc_matricula_epm"]] <- plotly_build(plot_ly(x = dataset$perc_matricula_epm, type = "histogram", name = "perc_matricula_epm"))
listOfPlots[["tx_distorcao_idade_serie_epf"]] <- plotly_build(plot_ly(x = dataset$tx_distorcao_idade_serie_epf, type = "histogram", name = "tx_distorcao_idade_serie_epf"))
listOfPlots[["tx_distorcao_idade_serie_epm"]] <- plotly_build(plot_ly(x = dataset$tx_distorcao_idade_serie_epm, type = "histogram", name = "tx_distorcao_idade_serie_epm"))
listOfPlots[["ideb_ai_ef"]] <- plotly_build(plot_ly(x = dataset$ideb_ai_ef, type = "histogram", name = "ideb_ai_ef"))
listOfPlots[["ideb_ai_em"]] <- plotly_build(plot_ly(x = dataset$ideb_ai_em, type = "histogram", name = "ideb_ai_em"))
listOfPlots[["perc_docente_epf"]] <- plotly_build(plot_ly(x = dataset$perc_docente_epf, type = "histogram", name = "perc_docente_epf"))
listOfPlots[["perc_docente_epm"]] <- plotly_build(plot_ly(x = dataset$perc_docente_epm, type = "histogram", name = "perc_docente_epm"))
listOfPlots[["tx_mortalidade_infantil"]] <- plotly_build(plot_ly(x = dataset$tx_mortalidade_infantil, type = "histogram", name = "tx_mortalidade_infantil"))
listOfPlots[["tx_mortalidade_bruta"]] <- plotly_build(plot_ly(x = dataset$tx_mortalidade_bruta, type = "histogram", name = "tx_mortalidade_bruta"))
listOfPlots[["perc_pop_cobertura_plano_saude"]] <- plotly_build(plot_ly(x = dataset$perc_pop_cobertura_plano_saude, type = "histogram", name = "perc_pop_cobertura_plano_saude"))
listOfPlots[["perc_internacao_falha_aps"]] <- plotly_build(plot_ly(x = dataset$perc_internacao_falha_aps, type = "histogram", name = "perc_internacao_falha_aps"))
listOfPlots[["perc_internacao_falha_saneamento"]] <- plotly_build(plot_ly(x = dataset$perc_internacao_falha_saneamento, type = "histogram", name = "perc_internacao_falha_saneamento"))
listOfPlots[["vl_per_capita_bolsa_familia"]] <- plotly_build(plot_ly(x = dataset$vl_per_capita_bolsa_familia, type = "histogram", name = "vl_per_capita_bolsa_familia"))
listOfPlots[["vl_per_capita_beneficio_prestacao_continuada"]] <- plotly_build(plot_ly(x = dataset$vl_per_capita_beneficio_prestacao_continuada, type = "histogram", name = "vl_per_capita_beneficio_prestacao_continuada"))
listOfPlots[["perc_pop_cadunico_sem_abastecimento_agua"]] <- plotly_build(plot_ly(x = dataset$perc_pop_cadunico_sem_abastecimento_agua, type = "histogram", name = "perc_pop_cadunico_sem_abastecimento_agua"))
listOfPlots[["perc_pop_cadunico_sem_esgotamento_sanitario"]] <- plotly_build(plot_ly(x = dataset$perc_pop_cadunico_sem_esgotamento_sanitario, type = "histogram", name = "perc_pop_cadunico_sem_esgotamento_sanitario"))
listOfPlots[["perc_pop_cadunico_sem_coleta_lixo"]] <- plotly_build(plot_ly(x = dataset$perc_pop_cadunico_sem_coleta_lixo, type = "histogram", name = "perc_pop_cadunico_sem_coleta_lixo"))
listOfPlots[["perc_pop_cadunico_sem_saneamento"]] <- plotly_build(plot_ly(x = dataset$perc_pop_cadunico_sem_saneamento, type = "histogram", name = "perc_pop_cadunico_sem_saneamento"))
listOfPlots[["perc_pop_pobre_cadunico"]] <- plotly_build(plot_ly(x = dataset$perc_pop_pobre_cadunico, type = "histogram", name = "perc_pop_pobre_cadunico"))
listOfPlots[["perc_pop_cadunico_bolsa_familia"]] <- plotly_build(plot_ly(x = dataset$perc_pop_cadunico_bolsa_familia, type = "histogram", name = "perc_pop_cadunico_bolsa_familia"))
listOfPlots[["perc_pop_urbana_com_abastecimento_agua"]] <- plotly_build(plot_ly(x = dataset$perc_pop_urbana_com_abastecimento_agua, type = "histogram", name = "perc_pop_urbana_com_abastecimento_agua"))
listOfPlots[["perc_participacao_queimadas_brasil"]] <- plotly_build(plot_ly(x = dataset$perc_participacao_queimadas_brasil, type = "histogram", name = "perc_participacao_queimadas_brasil"))
listOfPlots[["total_casos_confirmados_covid"]] <- plotly_build(plot_ly(x = dataset$total_casos_confirmados_covid, type = "histogram", name = "total_casos_confirmados_covid"))
listOfPlots[["total_obitos_confirmados_covid"]] <- plotly_build(plot_ly(x = dataset$total_obitos_confirmados_covid, type = "histogram", name = "total_obitos_confirmados_covid"))
listOfPlots[["cobertura_ab"]] <- plotly_build(plot_ly(x = dataset$cobertura_ab, type = "histogram", name = "cobertura_ab"))
listOfPlots[["incidencia"]] <- plotly_build(plot_ly(x = dataset$incidencia, type = "histogram", name = "incidencia"))
listOfPlots[["letalidade"]] <- plotly_build(plot_ly(x = dataset$letalidade, type = "histogram", name = "letalidade"))

```

Ploting...

```{r}

plotly::subplot(listOfPlots, nrows = 11)

```

As we can see in the plots above the first two variables could create problems in the models we intend to use. The third feature is in fact the population which could also create some bias. Said that, we gonna cut it off from dataset we will analyze.

## Data Position and Dispersion Measures

Here we should check the measurement about the position of the data such as the `min` and `max` values of each feature, the `mean`, `median` and `mode` as well.

All these stats could help us to implement a better model, adjusting the data to better outcomes from model. 

```{r}
getStats <- function(ds) {
  
  colNames <- names(ds)
  
  res <- data.frame()
  
  for(col in colNames){
    tmp <- data.frame(
      variable = col,
      min_value = min(ds[,col]),
      max_value = max(ds[,col]),
      range = max(ds[,col]) - min(ds[,col]),
      mean = mean(ds[,col]),
      median = median(ds[,col]),
      mode = mfv(ds[,col]),
      variance = var(ds[,col]),
      sd = sd(ds[,col]),
      mad = mad(ds[,col]),
      iqr = IQR(ds[,col]),
      first_quant = quantile(ds[,col], probs = c(.25)),
      third_quant = quantile(ds[,col], probs = c(.75))
    )
    
    res <- rbind(res, tmp)
  }
  
  return(res)
  
}
```


```{r}
dataset_stats <- getStats(dataset[,11:ncol(dataset)])

dataset_stats
```

As we can notice looking to the results, some features presents a multi mode behavior.
That's is interesting because it could help of classification or grouping.

Now, to understand better how the data interact among itself we gonna plot a correlogram.

```{r}
dataset_cor <- cor(dataset[,11:ncol(dataset)])
```

```{r}
colNames <- names(dataset[,11:ncol(dataset)])
plot_ly(
  x = colNames,
  y = colNames,
  z = dataset_cor, 
  type = "heatmap")
```

As we can see in the plot above, some features has a strong negative correlation and few of then has strong positive correlation. The first impressions is all the socio-economics indicators has nothing to do with the cases of Covid19.
Let's check it out!

# Target

As we can analyze the relation between socio-economics indicators and Covid19 to classify this relationship and we actually don't have a target feature we'll create this based on `lethality` and `incidence` as this variables show us how complicate the pandemic situation is taking in a specific location.

We setted up the following easy fuzzy rules.

For individual values of `incidence` and `lethality`:

- If the value is on the first quartile then **leve**
- If the value is on the second quartile then **moderado**
- If the value is on the third quartile then **grave**
- If the value is on the fourth quartile then **severo**

The labels have a degree of importance, going from an easy situation until a very complicated condition.
Then, looking for this definition, combining the individual labels into a single one will follow:

- If the distance between individual labels is 0 or 1 then choose the lower one
- If the distance between individual labels is 2 then increase the lower one by 1 level
- If the distance between individual labels is 3 then increase the lower one by 2 level

So...

```{r}
dataset <- dataset %>%
  mutate(label_incidencia = ifelse(incidencia <= quantile(incidencia, probs = c(.25)), "Leve",
                                   ifelse(incidencia > quantile(incidencia, probs = c(.25)) & incidencia <= quantile(incidencia, probs = c(.5)), "Moderado",
                                          ifelse(incidencia > quantile(incidencia, probs = c(.5)) & incidencia <= quantile(incidencia, probs = c(.75)), "Grave", "Severo"))),
         label_letalidade = ifelse(letalidade <= quantile(letalidade, probs = c(.25)), "Leve",
                                   ifelse(letalidade > quantile(letalidade, probs = c(.25)) & letalidade <= quantile(letalidade, probs = c(.5)), "Moderado",
                                          ifelse(letalidade > quantile(letalidade, probs = c(.5)) & letalidade <= quantile(letalidade, probs = c(.75)), "Grave", "Severo"))))
```

Combining...

```{r}
dataset <- dataset %>%
  mutate(label = ifelse(label_incidencia == label_letalidade, label_incidencia,
                        ifelse((label_incidencia == "Leve" & label_letalidade == "Moderado") | (label_incidencia == "Moderado" & label_letalidade == "Leve"), "Moderado",
                               ifelse((label_incidencia == "Leve" & label_letalidade == "Grave") | (label_incidencia == "Grave" & label_letalidade == "Leve"), "Moderado",
                                      ifelse((label_incidencia == "Leve" & label_letalidade == "Severo") | (label_incidencia == "Severo" & label_letalidade == "Leve"), "Grave",
                                             ifelse((label_incidencia == "Moderado" & label_letalidade == "Grave") | (label_incidencia == "Grave" & label_letalidade == "Moderado"), "Moderado",
                                                    ifelse((label_incidencia == "Moderado" & label_letalidade == "Severo") | (label_incidencia == "Severo" & label_letalidade == "Moderado"), "Grave","Grave")))))))
```

Getting some stats from the labels

```{r}
describe(dataset$label)
```

As we can see the label **Grave** answer by 53% of all dataset, followed by **Severo** with 36%, **Leve** with 8% and **Severo** with 2%.

So before we start to train some model we need to equalize the numbers to create a fair situation to the model.

# Train

For the first test we won't change any feature. 
We gonna try to train some algorithms and see what we get back as answer.
Depending on the results we'll change or remove some features. There is no hard and fast rule to tell you when and how modify your data. You can always start by fitting your model to raw and after normalize and/or standardize data and compare the performance for best results.

But before to start we need to create a train dataset equalized, with the same number of occurrences for the target feature.
As we already saw before, we have an unbalanced dataset.

We just need to keep only the features already tested and the target as the other columns does not help us in any way.

If that said, we first will select only the features we'll use and after that we'll use some techniques to balance the number of labels.

```{r}
set.seed(100)
```

Selecting the data to train:

```{r}
dataset_train <- dataset %>%
  select(11:ncol(dataset), -total_casos_confirmados_covid, -total_obitos_confirmados_covid, -label_incidencia, -label_letalidade) %>%
  droplevels()
```

Saving the dataset selected to future checking process.

```{r}
write.csv2(dataset_train, 'dataset_train.csv', row.names = F)
```


Spliting the data using the rule 60/20/20

```{r}
dataset_train$label <- as.factor(dataset_train$label)
trainRowNumbers <- createDataPartition(dataset_train$label, p=0.6, list=FALSE)

trainData <- dataset_train[trainRowNumbers,]

dataset_test <- dataset_train[-trainRowNumbers,]

testRowNumbers <- createDataPartition(dataset_test$label, p=0.5, list=FALSE)

devData <- dataset_test[testRowNumbers,]
testData <- dataset_test[-testRowNumbers,]
```

Is in the `trainData` where we should apply the balance technique.

```{r}
describe(trainData$label)
```

Let's apply the SMOTE technique to oversampling the minors levels.

```{r}
trainData_smoted <- SMOTE(label ~ ., trainData, perc.over = 1000, perc.under = 300)
describe(trainData_smoted$label)

trainData_smoted <- SMOTE(label ~ ., trainData_smoted, perc.over = 1000, perc.under = 300)
describe(trainData_smoted$label)
```

As some testes showed the SMOTE approach was not able to handle all variables at the same way. Thus, we decide to apply twice, one time over the `trainData` and the second over the SMOTEd data. 
Thereby, we could get a more balanced dataset to use to train the models.

Applying 

## Decision Tree Model

Here we define a Cross-Validation using 10 folds of data and 3 repetition.

```{r}
trainCtrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```

```{r}
set.seed(100)

dtree_fit <- train(label ~., data = trainData_smoted, method = "rpart",
                   parms = list(split = "information"),
                   trControl=trainCtrl,
                   tuneLength = 10)
```

Checking results

```{r}
dtree_fit
```

Let's check how the tree was built.

```{r}
prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```

Testing the prediction with `devData`...

```{r}
dtree_test <- predict(dtree_fit, newdata = devData)
confusionMatrix(dtree_test, devData$label )  
```

We found such good results, but now we gonna test using the `testData`.

```{r}
dtree_test <- predict(dtree_fit, newdata = testData)
confusionMatrix(dtree_test, testData$label ) 
```

Well, not only with the `devData` but also with `testData` we could found. In both the outcome bring us 99.89% of Accuracy and Kappa value equals to 99,81%.

Accuracy means how many correct answers the model could give based on the dataset which was used to train.

Kappa statistic is a very good measure that can handle very well both multi-class and imbalanced class problems. There is no standardized way to interpret its values. Landis and Koch (1977) provide a way to characterize values. According to their scheme a value < 0 is indicating no agreement , 0–0.20 as slight, 0.21–0.40 as fair, 0.41–0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect agreement.

## Random Forest

Now we gonna try another machine learning technique. Random Forest uses almost the same approach as the Decision Tree but its actually creates a "forest", that is its creates a large amount of trees and use they together to refine the results.

We gonna start with 500 trees and depending on the outcomes we return here to refine it.

```{r}
trainCtrl <- trainControl(method = "cv", classProbs = TRUE, summaryFunction = multiClassSummary, number = 3)

set.seed(100)

rf_fit <- train(label ~., data = trainData_smoted, method = "rf",
                ntree = 500, tunelength = 10, metric = "logLoss",
                trControl = trainCtrl,
                importance = TRUE)
```

So, let's check it out

```{r}
rf_fit
```

Testing with the `devData`

```{r}
rf_test <- predict(rf_fit, newdata = devData)
confusionMatrix(rf_test, devData$label)  
```

And now with `testData`

```{r}
rf_test <- predict(rf_fit, newdata = testData)
confusionMatrix(rf_test, testData$label)  
```

Using the Random Forest technique we also could find some good results. For the first test we got 99.57% of Accuracy and Kappa value equals to 99.25%. At the second test we got 99.78% of Accuracy and 99.63% as Kappa.

## KNN

For now, we'll try the KNN algorithm and check if we can get the same level of outcomes.

```{r}
trainCtrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

set.seed(100)

knn_fit <- train(label ~., data = trainData_smoted, method = "knn",
                trControl = trainCtrl,
                preProcess = c("center", "scale"),
                tuneLength = 10)
```

Here we also used the Cross-Validation approach to refine the results as we did with Decision Tree before.

```{r}
knn_fit
```

As we can see above, the best K value was 5 which can give us 86.99% of Accuracy and 82.53% as Kappa value. The K value means the number of clusters deliver the best results.

Let's check with the `devData`

```{r}
knn_test <- predict(knn_fit, newdata = devData)
confusionMatrix(knn_test, devData$label)  
```

And now with `testData`

```{r}
knn_test <- predict(knn_fit, newdata = testData)
confusionMatrix(knn_test, testData$label)  
```

Quite different from what was expected KNN results where the worst. Even with the model trained giving the 86.99% of Accuracy and 82.53% of Kappa value the model does not fitted very well with the test sets of data. This probably occurred due to the dataset, as we could saw with histograms maybe the data needs some pre processing to be standardized  

For classification algorithms like KNN, we measure the distances between pairs of samples and these distances are influenced by the measurement units also. 

For our dataset we are applying KNN on a data having 26 features. We have feature with range of 5.3 units and features where the range is 20723 units. In this case, most of the clusters will be generated based on features with largest ranges. To avoid this miss classification, we should normalize the feature variables. Actually, any algorithm where distance play a vital role for prediction or classification, we should normalize the variable.

### Feature Scaling

Before we start the feature scaling process we should understand a little about this process and you could have some references [HERE](https://towardsdatascience.com/normalization-vs-standardization-cb8fe15082eb) and [HERE](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/). It's also crucial understand how your algorithm works first. In the case of cluster algorithms we must prioritize similarities between features based on distance measures.

When we **Normalize** a dataset we actually distribute data between the range [0,1]. This option is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.

But, when we **Standardize** the data, we distribute data centered around 0 with the range between [-1,1]. This option, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.


For our situation here we will use **normalization** approach.

```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```


```{r}
dataset_train_scaled <- dataset_train

dataset_train_scaled[,1:26] <- normalize(dataset_train_scaled[,1:26])
```


```{r}
trainRowNumbers <- createDataPartition(dataset_train_scaled$label, p=0.6, list=FALSE)

trainData_scaled <- dataset_train_scaled[trainRowNumbers,]

dataset_test_scaled <- dataset_train_scaled[-trainRowNumbers,]

testRowNumbers <- createDataPartition(dataset_test_scaled$label, p=0.5, list=FALSE)

devData_scaled <- dataset_test_scaled[testRowNumbers,]
testData_scaled <- dataset_test_scaled[-testRowNumbers,]
```

```{r}
describe(trainData_scaled$label)
```

```{r}
trainData_scaled_smoted <- SMOTE(label ~ ., trainData_scaled, perc.over = 1000, perc.under = 300)
describe(trainData_scaled_smoted$label)

trainData_scaled_smoted <- SMOTE(label ~ ., trainData_scaled_smoted, perc.over = 1000, perc.under = 300)
describe(trainData_scaled_smoted$label)
```


```{r}
trainCtrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

set.seed(100)

knn_fit <- train(label ~., data = trainData_scaled_smoted, method = "knn",
                trControl = trainCtrl,
                preProcess = c("center", "scale"),
                tuneLength = 10)
```

```{r}
knn_fit
```

The model gave us almost the same result. The algorithm choose the same K value and the Accuracy and Kappa values was pretty nearly the same.

Let's check if the feature scaling process was able to improve our results.

```{r}
knn_test <- predict(knn_fit, newdata = devData_scaled)
confusionMatrix(knn_test, devData_scaled$label)  
```

Now with the `testData_scaled`

```{r}
knn_test <- predict(knn_fit, newdata = testData_scaled)
confusionMatrix(knn_test, testData_scaled$label)  
```


# Conclusion

A Decision Tree is a decision support tool that uses a tree-like model of decisions taking into account their possible consequences, chance of an event happen, costs and utility. On the other hand, a Random Forest is an ensemble learning method that builds a forest of decision trees during the training process and gives the final outcome based on the outputs of all its decision trees.

A Decision Tree is invariant to feature scaling as well as Random Forests so. Is this both situation we hadn't any need for changes in dataset.

Unlike this two first trials, the KNN approach results was not as expected. During the first round using KNN we applied the wad data as we did with Decision Tree and Random Forest. Reading more about the algorithm we realized the data should be scaled as clustering algorithm is particularly sensitive to range variations. As you can see above a normalization of all features was applied and and a new attempt was made but with the same results.

Despite the good results using the first two algorithms, maybe the dataset is not "good enough" to KNN once we saw in correlogram the poor relationship among the most features. However, taking this as the only data we have to work, we must discard KNN as an option and decide between DT or RF.

There is a possibility of overfitting in a decision tree. The use of multiple trees in the random forest reduces the risk of overfitting. A random forest gives more accurate results than a decision tree.

With all this said and looking for the results we classify the results as:

1. Random Forest
2. Decision Tree
3. KNN













